# -*- coding: utf-8 -*-
"""Ongky Arif Wibowo_Kalbe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L4zh7ParD2eVuGIQslhSQhLDntNR1J25

# Environment Setup

## Library Setup
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima.model import ARIMA
import pmdarima as pm
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_samples, silhouette_score, mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error
from sklearn.model_selection import TimeSeriesSplit
from yellowbrick.cluster import KElbowVisualizer

import warnings
warnings.filterwarnings('ignore')

"""## Daset Setup"""

dfCustomer = pd.read_csv('Dataset/Case Study - Customer.csv', sep= ';')
dfProduct = pd.read_csv('Dataset/Case Study - Product.csv', sep= ';')
dfStore = pd.read_csv('Dataset/Case Study - Store.csv', sep= ';')
dfTransaction = pd.read_csv('Dataset/Case Study - Transaction.csv', sep= ';')

"""# Exploratory Data Analysis

## Function
"""

def statisticDescription (data):
    print('5 Data Sample :')
    print(data.head(5))
    print('\n\n\nInformation Data :')
    print(data.info())
    print('\n\n\nData Shape : \n', data.shape)
    print('\n\n\nMissing Value Total : \n', data.isna().sum())
    print('\n\n\nDuplicate Value Total : \n', data.duplicated().sum())
    print('\n\n\nDescriptive Statistic :')
    return data.describe(include='all')

"""## Customer"""

statisticDescription(dfCustomer)

"""## Product"""

statisticDescription(dfProduct)

"""## Store"""

statisticDescription(dfStore)

"""## Transaction"""

statisticDescription(dfTransaction)

"""# Merge Data"""

df = pd.merge(dfTransaction, dfCustomer, on='CustomerID', how='inner')
df2 = pd.merge(df, dfStore, on = 'StoreID', how = 'inner')
dfMerged = pd.merge(df2, dfProduct, on = 'ProductID', how = 'inner')
dfMerged.drop(columns = "Price_y", inplace = True)
statisticDescription(dfMerged)

"""# Handling Missing Value"""

print('Perbandingan missing value ',round((dfMerged['Marital Status'].isna().sum()/dfMerged.shape[0])*100, 2),'% dari total data')

dfMerged.dropna(inplace = True)
dfMerged.isna().sum()

"""# Data Transformation"""

dfMerged['Date'] = pd.to_datetime(dfMerged['Date'], format='%d/%m/%Y')

dfMerged['Date'].head()

"""# Arima Regression Prediction (Forecasting)

## Data Preparation
"""

targetRegression = dfMerged.groupby('Date').agg({'Qty': 'sum'}).reset_index()

targetRegression.head()

print(targetRegression.shape)

"""## Stationarity Check"""

fig, ax = plt.subplots(figsize=(15, 5))
sns.lineplot(x = 'Date', y = 'Qty', data = targetRegression)
plt.tight_layout()

targetRegressionIdx = targetRegression.set_index('Date')
regresi_decomposition = seasonal_decompose(targetRegressionIdx)

# Plot the decomposition results
fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize = (15, 10), sharex = True)
regresi_decomposition.observed.plot(ax = ax1)
ax1.set_ylabel('Observed')
regresi_decomposition.trend.plot(ax = ax2)
ax2.set_ylabel('Trend')
regresi_decomposition.seasonal.plot(ax = ax3)
ax3.set_ylabel('Seasonal')
regresi_decomposition.resid.plot(ax = ax4)
ax4.set_ylabel('Residual')

plt.tight_layout()

fig, ax = plt.subplots(1, 2, figsize = (15,5))

plot_acf(targetRegression['Qty'], ax = ax[0])
ax[0].set_title('Autocorrelation Function (ACF)')

plot_pacf(targetRegression['Qty'], ax = ax[1])
ax[1].set_title('Partial Autocorrelation Function (PACF)')

plt.tight_layout()

checkStationery = adfuller(targetRegression['Qty'], autolag = 'AIC')

print("1. ADF : ",checkStationery[0])
print("2. P-Value : ", checkStationery[1])
print("3. Num Of Lags : ", checkStationery[2])
print("4. Num Of Observations Used For ADF Regression:", checkStationery[3])
print("5. Critical Values :")
for key, val in dftest[4].items():
    print("\t",key, ": ", val)

alpha = 0.05
if checkStationery[1] <= alpha:
    print('\n\nThe data is stationary')
    print(checkStationery[1])
else:
    print('\n\nThe data is not stationary')
    print(checkStationery[1])

"""## Splitting Dataset"""

train_size = int(len(targetRegression) * 0.8)
train_data, test_data = targetRegression.iloc[:train_size], targetRegression.iloc[train_size:]

print('Train Data Size : ', trainData.shape)
print('Test Data Size : ', testData.shape)

plt.figure(figsize=(15,5))
sns.lineplot(data=train_data, x='Date', y='Qty', label='Train')
sns.lineplot(data=test_data, x='Date', y='Qty', label='Test')
plt.legend()

print(train_data)

print(test_data)

"""## Determine Model & Best Paramter
(ARI, ARIMA, IMA)
"""

from itertools import product
p = range(4)
q = range(4)
d = range(1)
pdq = list(product(p, d, q))
print(pdq)

aic_scores = []
for param in pdq:
    model = ARIMA(train_data['Qty'], order=param)
    model_fit = model.fit()
    aic_scores.append({'Parameter': param, 'AIC Score': model_fit.aic})
    print(model_fit.summary())


best_aic = min(aic_scores, key=lambda x: x['AIC Score'])
print('\n\n\n Best Model : ',best_aic)

def rmse(y_actual, y_pred):
  print(f'RMSE Value: {mean_squared_error(y_actual, y_pred)**0.5}')
def rsquare(y_actual, y_pred):
  print(f'R-squared Value: {r2_score(y_actual, y_pred)}')
def eval(y_actual, y_pred):
  rmse(y_actual, y_pred)
  rsquare(y_actual, y_pred)
  print(f'MAE Value: {mean_absolute_error(y_actual, y_pred)}')

"""## Train Testing Model"""

order = (2,0,2)
model = sm.tsa.ARIMA(train_data['Qty'], order = order)
fit_qty = model.fit()

print(fit_qty.summary())

# ARIMA (0,0,0) Seasonal (0,1,1,7)
df_train = train_data.set_index('Date')
df_test = test_data.set_index('Date')

y_pred = fit_qty.get_forecast(len(df_test))

y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = fit_qty.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])
y_pred_df.index = df_test.index
y_pred_out = y_pred_df['predictions']

eval(df_test['Qty'], y_pred_out)

plt.figure(figsize = (10,8))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color = 'red')
plt.plot(y_pred_out, color = 'black', label = 'ARIMA Predictions')
plt.legend()

plt.tight_layout()

"""## Forecasting"""

forecast_length = 31
forecast_result = fit_qty.get_forecast(forecast_length)
forecast_result_arima = forecast_result.conf_int()
forecast_result_arima['forecasted Qty'] = fit_qty.predict(start = forecast_result_arima.index[0],
                                                      end = forecast_result_arima.index[-1])
forecast_result_arima['Date'] = pd.date_range(start = '2023-01-01', end = '2023-01-31')
forecast_result_arima.set_index('Date', inplace = True)
forecast_result_arima.head()

plt.figure(figsize = (10,8))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color = 'red')
plt.plot(y_pred_out, color = 'black', label = 'ARIMA Predictions')
plt.plot(forecast_result_arima['forecasted Qty'], color = 'yellow', label = 'ARIMA Forecast')
plt.legend()

plt.tight_layout()

fig, ax = plt.subplots(figsize=(15, 5))
sns.lineplot(x = 'Date', y = 'forecasted Qty', data = forecast_result_arima)
plt.tight_layout()

"""# KMeans Clustering

## Data Preparation

### Merge Data
"""

df_cluster = dfMerged.groupby('CustomerID').agg({'TransactionID':'count','Qty':'sum','TotalAmount':'sum'}).reset_index()

df_cluster.head()

"""### Data Transformation"""

df_fix = df_cluster.drop('CustomerID', axis = 1)
num_fix = df_fix.columns

df_norm = MinMaxScaler().fit_transform(df_fix)
df_norm = pd.DataFrame(data = df_norm, columns = num_fix)
df_norm.head()

"""### Exploratory Data"""

ax = sns.heatmap(df_norm.corr(), annot=True)

plt.figure(figsize = (15,5))
for i in range(0, len(num_fix)):
  plt.subplot(1, 3, i + 1)
  sns.distplot(df_norm[num_fix[i]], color = 'red')
  plt.xlabel(num_fix[i])
  plt.tight_layout()

statisticDescription(df_norm)

"""### Determine Cluster K"""

inertia = []
for i in range(1,11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 42)
    kmeans.fit(df_norm.values)
    inertia.append(kmeans.inertia_)

sns.lineplot(x = range(1,11), y = inertia, linewidth = 4)
sns.scatterplot(x = range(1,11), y = inertia, s = 60)
plt.tight_layout()

model = KMeans(random_state=42)
checkDistortion = KElbowVisualizer(model, k = (2,11), metric = 'distortion', timings = True, locate_elbow = True)
checkDistortion.fit(df_norm)
checkDistortion.show()

model = KMeans(random_state = 42)
visualizer = KElbowVisualizer(model, k = (2,11), metric = 'silhouette', timings = True, locate_elbow = True)
visualizer.fit(df_norm)
visualizer.show()

# silhouette plot
from yellowbrick.cluster import SilhouetteVisualizer

for i in range(2,6):
    model = KMeans(i, random_state = 42)
    visualizer = SilhouetteVisualizer(model, colors = 'yellowbrick')
    visualizer.fit(df_norm)
    visualizer.show()

for num_clusters in range(2,6):
    #inisialisasi kmeans
    model_clus = KMeans(n_clusters = num_clusters, max_iter = 1000, random_state = 42)
    model_clus.fit(df_norm)

    cluster_labels = model_clus.labels_

    #shilhouette score
    silhouette_avg = silhouette_score(df_norm, cluster_labels)
    print("For n_clusters = {0}, the silhouette score is {1}".format(num_clusters, silhouette_avg))

"""### Train Test Model"""

kmeans = KMeans(n_clusters = 3, max_iter = 300, random_state = 42)
kmeans.fit(df_norm.values)

df_norm['label'] = kmeans.labels_

df_norm.head(10)

"""### Visualize Data Distribution"""

X = df_norm.copy().drop(['label'], axis = 1)
Y = df_norm['label'].copy()

scaler = MinMaxScaler()
scaler.fit(X)
X_norm = scaler.transform(X)

pca = PCA()
pca.fit(X_norm)

plt.plot(range(1,4), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--')
plt.title('Explained Variance by Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.show()

pca = PCA(n_components = 2)
pca.fit(X_norm)
X_pca = pca.transform(X_norm)

df_cluster = pd.DataFrame(X_pca, columns = ['Feature 1', 'Feature 2'])
df_cluster['label'] = Y
df_cluster.sample(10)

df_cluster['label'].value_counts()

df_cluster.describe()

df_cluster.groupby('label').agg(['mean', 'median', 'std', 'count'])

sns.scatterplot(data = df_cluster, x = 'Feature 1', y = 'Feature 2', hue = 'label', palette='RdYlBu')
plt.show()

df_fix['cluster'] = kmeans.labels_
df_fix.groupby('cluster').agg(['count','mean', 'median', 'std'])

df_fix.describe().T

cluster_count = df_fix['cluster'].value_counts().reset_index()
cluster_count.columns = ['cluster', 'count']
cluster_count['percentage (%)'] = round((cluster_count['count']/len(df_fix))*100,2)
cluster_count = cluster_count.sort_values(by = ['cluster']).reset_index(drop = True)
cluster_count

fig, ax = plt.subplots(figsize = (15,5))

charts = plt.bar(x = cluster_count['cluster'], height = cluster_count['percentage (%)'], color = ["#ffa600", "#58508d", "#bc5090", "#ff6361", "#003f5c"])

for chart in charts:
  height = chart.get_height()
  label_x_pos = chart.get_x() + chart.get_width() / 2
  ax.text(label_x_pos, height, s = f'{height} %', ha='center',
  va = 'bottom')

plt.title('Percentage of Customer by Cluster', fontsize = 16)
plt.xlabel('Cluster',fontsize = 12)
plt.ylabel('Percentage',fontsize = 12)
plt.style.use('tableau-colorblind10')
plt.grid(False)
plt.tight_layout()

cluster_med = df_norm.groupby('label').mean().reset_index()

df_melt = pd.melt(cluster_med.reset_index(), id_vars='label', value_vars = num_fix, var_name = 'Metric', value_name = 'Value')

plt.figure(figsize=(10,6))
sns.pointplot(data = df_melt, x = 'Metric', y = 'Value', hue = 'label', palette = ["#ffa600", "#58508d", "#bc5090", "#ff6361", "#003f5c"])
plt.title('Pattern of Customer by KMeans Clustering Model', fontsize = 14)
plt.xlabel('Metric')
plt.ylabel('Value')
plt.tight_layout()

plt.figure(figsize = [len(num_fix)*4,3])
i = 1
for col in num_fix:
    ax = plt.subplot(1,len(num_fix),i)
    ax.vlines(df_fix[col].mean(), ymin = -0.5, ymax = 2.5, color = 'black', linestyle='--')
    g = df_fix.groupby('cluster')
    x = g[col].mean().index
    y = g[col].mean().values
    ax.barh(x,y, color = ["#ffa600", "#58508d", "#bc5090", "#ff6361", "#003f5c"])
    plt.title(col)
    plt.grid(False)
    i = i+1

df_fix.groupby('cluster').agg(['min', 'max', 'mean']).reset_index(drop = True).T